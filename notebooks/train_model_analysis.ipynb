{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, (Path(\".\").resolve().parent / \"common\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = Path(\".\").resolve().parent / \"input/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/overfit config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from albumentations import Compose, RandomCrop, RandomCropNearBBox, ShiftScaleRotate, GaussNoise, ElasticTransform\n",
    "from albumentations import CenterCrop\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "from dataflow.dataloaders import get_base_train_val_loaders_by_fold, HPADataset\n",
    "from models.resnet import HPAResNet50\n",
    "\n",
    "from custom_ignite.metrics.accuracy import Accuracy\n",
    "from custom_ignite.metrics.precision import Precision\n",
    "from custom_ignite.metrics.recall import Recall\n",
    "\n",
    "\n",
    "seed = 12\n",
    "device = \"cuda\"\n",
    "debug = True\n",
    "\n",
    "val_fold_index = 0\n",
    "\n",
    "\n",
    "train_transforms = Compose([\n",
    "    # ShiftScaleRotate(shift_limit=0.2, scale_limit=0.01, rotate_limit=15, interpolation=cv2.INTER_CUBIC, p=0.3),\n",
    "    # ElasticTransform(p=0.3),\n",
    "    CenterCrop(256, 256),\n",
    "    ToTensor()\n",
    "])\n",
    "train_transform_fn = lambda dp: train_transforms(**{\"image\": dp[0], \"tags\": dp[1].astype('float32')})\n",
    "\n",
    "\n",
    "val_transforms = Compose([\n",
    "    CenterCrop(250, 250),\n",
    "    ToTensor()\n",
    "])\n",
    "val_transform_fn = lambda dp: val_transforms(**{\"image\": dp[0], \"tags\": dp[1].astype('float32')})\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "train_loader, val_loader = get_base_train_val_loaders_by_fold(INPUT_PATH, train_transform_fn, val_transform_fn,\n",
    "                                                              batch_size=batch_size, num_workers=8,\n",
    "                                                              fold_index=val_fold_index, n_splits=3,\n",
    "                                                              random_state=seed,\n",
    "                                                              limit_train_num_samples=5,\n",
    "                                                              limit_val_num_samples=5)\n",
    "\n",
    "model = HPAResNet50(num_classes=HPADataset.num_tags)\n",
    "\n",
    "\n",
    "# Training\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Optional config param\n",
    "# lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * 10, eta_min=1e-5)\n",
    "lr_scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80], gamma=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "def thresholded_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "    return y_pred, y\n",
    "\n",
    "\n",
    "# Optional config param\n",
    "metrics = {\n",
    "    \"precision\": Precision(output_transform=thresholded_output_transform,\n",
    "                           average=True, is_multilabel=True),\n",
    "    \"recall\": Recall(output_transform=thresholded_output_transform,\n",
    "                     average=True, is_multilabel=True),\n",
    "    \"accuracy\": Accuracy(output_transform=thresholded_output_transform,\n",
    "                         is_multilabel=True)\n",
    "}\n",
    "\n",
    "log_interval = 10\n",
    "val_interval_epochs = 1\n",
    "val_metrics = metrics\n",
    "\n",
    "trainer_checkpoint_interval = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch['image'].cuda()\n",
    "y = batch['tags'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torch.cuda.FloatTensor',\n",
       " torch.Size([5, 4, 256, 256]),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(1., device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type(), x.shape, x.min(), x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torch.cuda.FloatTensor',\n",
       " torch.Size([5, 28]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.type(), y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torch.cuda.FloatTensor',\n",
       " torch.Size([5, 28]),\n",
       " tensor([[ 5.1568e-01, -6.6601e-01,  5.0193e-01, -3.3513e-01, -8.4777e-01,\n",
       "           1.5676e+00,  4.1978e-02,  3.5155e-01, -7.3856e-01, -4.2269e-02,\n",
       "           1.0408e-01,  4.6202e-01, -1.4127e-01,  1.3753e-01,  2.8624e-02,\n",
       "          -5.8597e-01,  6.2380e-03, -1.1810e+00,  2.8056e-01, -5.4474e-01,\n",
       "          -3.3011e-01,  4.6405e-02, -8.7605e-01, -1.2937e+00, -4.7177e-01,\n",
       "           2.3426e-01, -2.0903e-01,  1.3883e+00],\n",
       "         [-5.6634e-01, -1.8244e-01,  3.3004e-01, -1.8493e-01, -4.8620e-01,\n",
       "           1.2350e+00, -1.2161e-01,  1.3512e-01, -1.2992e-01,  1.0296e-01,\n",
       "          -2.5559e-01, -4.2026e-02, -8.0712e-02, -1.9398e-01,  5.0062e-01,\n",
       "          -1.4312e-01, -2.4352e-01, -8.8529e-01,  1.4527e-01, -2.6481e-01,\n",
       "          -4.2727e-01,  2.0227e-01, -7.9223e-01, -6.2631e-01, -2.5884e-01,\n",
       "          -2.2482e-01, -3.9603e-02,  9.5804e-01],\n",
       "         [-3.9562e-01, -2.2092e-01,  2.4799e-01, -2.4404e-01, -4.6749e-01,\n",
       "           1.2710e+00, -1.2000e-01,  1.2077e-01, -2.2513e-01,  1.5094e-03,\n",
       "          -1.9373e-01,  6.5477e-02,  1.8921e-02, -1.1806e-01,  3.8900e-01,\n",
       "          -1.4967e-01, -2.2395e-01, -9.8671e-01,  1.3469e-01, -2.5370e-01,\n",
       "          -4.3504e-01,  1.8449e-01, -8.3927e-01, -6.4629e-01, -2.3021e-01,\n",
       "          -1.6141e-01, -1.0587e-02,  1.1333e+00],\n",
       "         [ 6.9849e-02, -5.1882e-01,  3.1900e-01, -2.9729e-01, -7.1852e-01,\n",
       "           1.6312e+00, -5.2357e-02,  2.2615e-01, -7.1196e-01,  3.0556e-02,\n",
       "           1.2272e-02,  4.4077e-01,  1.1626e-01,  2.9181e-02,  5.1138e-02,\n",
       "          -5.5888e-01, -1.2397e-02, -8.7888e-01,  1.8871e-01, -5.2101e-01,\n",
       "          -3.8603e-01,  1.1607e-01, -1.2193e+00, -1.2910e+00, -4.1529e-01,\n",
       "           2.5514e-01, -1.2355e-01,  1.4347e+00],\n",
       "         [ 4.5028e-01, -7.7843e-01,  6.4595e-01, -6.4580e-02, -7.8174e-01,\n",
       "           1.4291e+00,  3.7840e-03,  1.5478e-01, -7.0688e-01, -1.1184e-01,\n",
       "           2.7516e-02,  4.8024e-01,  2.4604e-01,  7.9377e-02, -7.5188e-02,\n",
       "          -5.2398e-01, -2.2167e-01, -1.1207e+00,  1.7953e-01, -5.3516e-01,\n",
       "          -2.1286e-01, -4.5465e-02, -1.2300e+00, -1.2321e+00, -4.5129e-01,\n",
       "           3.5204e-01, -5.4070e-02,  1.5172e+00]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.type(), y_pred.shape, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MultiLabelMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(y_pred, y.type(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.1940, device='cuda:0', grad_fn=<MultilabelMarginLossBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_planes = 512\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(in_planes, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.ConvTranspose2d(256, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 80, 80])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((8, 512, 40, 40))\n",
    "\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
